---
editor_options: 
  markdown: 
    wrap: 72
---

# NGS sequence analysis

```{r , echo=FALSE, eval=TRUE, include=TRUE}
klippy::klippy(c('r', 'bash'), position = c('top', 'right'), tooltip_message = 'copy to clipboard', tooltip_success = 'Copied!')
```

## Background on high throughput sequencing

High-throughput sequencing, also known as massively parallel sequencing
or next-generation sequencing (NGS), is a collection of methods and
technologies that can sequence DNA thousands/millions of fragments at a
time. The market leader on NGS is Illumina, and an overview of their
technology is in the video below.

<iframe width="560" height="315" src="https://www.youtube.com/embed/fCd6B5HRaZ8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>

</iframe>

There are many uses for high throughput sequencing including:

-   Whole genome sequencing

-   Amplicon sequencing - PCR of a targeted gene(s) is step one

    -   environmental DNA
    -   16S Bacterial community analysis
    -   Targeted gene panels

-   RNA sequencing

-   ChIP sequencing: Protein-DNA interaction analysis

Importantly a **lot** of the basic bioinformatics is the same across
these technologies, because the data that is produced from the
sequencing run is basically the same as well. The *big* data generated
here are all massive *FASTQ* files, processing these follows basically
the same initial pipeline for all applications.

## Some terms

```{r,  echo=FALSE, eval=TRUE}
terms <- read_csv("data/sequencing.csv", skip_empty_rows = TRUE) 

knitr::kable(terms, booktabs = TRUE,
             col.names = c("Term", "What it is"),
             longtable=TRUE) %>%
  kableExtra::row_spec(0, bold = TRUE) %>%
  kableExtra::kable_styling()
```

## The data

This data comes from exploring an underwater mountain \~3 km down at the
bottom of the Pacific Ocean that serves as a low-temperature (\~5-10Â°C)
hydrothermal venting site.

This amplicon dataset was generated from DNA extracted from crushed
basalts collected from across the mountain with the goal being to begin
characterizing the microbial communities of these deep-sea rocks. No one
had ever been here before, so this was a broad-level community survey.
The sequencing was done on the Illumina MiSeq platform with 2x300
paired-end sequencing using primers targeting the V4 region of the 16S
rRNA gene.

There are 20 samples total: 4 extraction "blanks" (nothing added to DNA
extraction kit), 2 bottom-water samples, 13 rocks, and one biofilm
scraped off of a rock. None of these details are important for you to
remember, it's just to give some overview if you care.

<details>

<summary>**Q. Why would we include "blank" samples in our sequencing
run? - Click here for Answer**</summary>

*This sort of "environmental data" is very at risk of contamination,
although the DNA extractions, and PCRs have to be run under sterile
conditions or they will pick up bacteria from the lab and not the
sample. Despite our best efforts we can still get minor contamination,
these "blank" runs can be useful as anything in these samples*
**cannot** *have come from our deep-sea rocks, and therefore we could
choose to "remove" sequences that match these in our other samples and
label them as contamination.*

</details>

In the following figure, overlain on the map are the rock sample
collection locations, and the panes on the right show examples of the 3
distinct types of rocks collected: 1) basalts with highly altered, thick
outer rinds (\>1 cm); 2) basalts that were smooth, glassy, thin
exteriors (\~1-2 mm); and 3) one calcified carbonate.

```{r, eval=TRUE,echo=FALSE,  nice-fig, fig.cap='Map of collection sites and examples of the rocks collected', out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/samples.png")
```

Altogether the uncompressed size of the working directory we are
downloading here is \~300MB - this is about 10% of the full dataset - we
are using a reduced dataset to minimise system requirements and speed up
the workflow.

To get started, be sure you are in the "Terminal" window. We will be
working here for the first step of importing the data, and removing the
primers from our data. We can import our data using the `curl` function,
we will then remove the primers using a program called `cutadapt` which
is written in Python.

Make sure when you open the terminal you are in the project directory
(and refer to last weeks notes if you need to check how to do this).

Don't switch over to R (the "Console" tab in the Binder/RStudio
environment) until noted. You can download the required dataset and
files by copying and pasting the following commands into your
command-line terminal:

```{bash}
    curl -L -o dada2_amplicon_ex_workflow.tar.gz https://ndownloader.figshare.com/files/23066516
    tar -xzvf dada2_amplicon_ex_workflow.tar.gz
    rm dada2_amplicon_ex_workflow.tar.gz
    cd dada2_amplicon_ex_workflow/
```

<details>

<summary>**Q. Can you work out what each of these lines of code might be
doing? - Click here for Answer**</summary>

*In brief these commmands:*

*- download/curl some external data* *- uncompress into a folder* *-
remove the compressed file* *- change the working directory to the newly
created folder*

</details>

In our working directory there are now 20 samples with one forward (R1)
and one reverse (R2) read each, each file has DNA sequences with
per-base-call quality information, for a total of 40 fastq files (.fq).
It is a good idea to have a file with all the sample names to use for
various things throughout, so here's making that file based on how these
sample names are formatted.

```{bash}
    ls *_R1.fq | cut -f1 -d "_" > samples
```

### FASTQ?

FASTQ format is a text-based format for storing both a biological
sequence (usually nucleotide sequence) and its corresponding quality
scores. As each nucleotide in a *read* is sequenced, it is assigned a
*Phred quality score*. This score is the assigned *probability* of the
sequencer having made an incorrect base call

```{r,  echo=FALSE, eval=TRUE}
terms <- read_csv("data/phred.csv", skip_empty_rows = TRUE) 

knitr::kable(terms, booktabs = TRUE,
             col.names = c("Phred Quality Score", "Probability of incorrect base call", "Base call accuracy"),
             longtable=TRUE) %>%
  kableExtra::row_spec(0, bold = TRUE) %>%
  kableExtra::kable_styling()
```

These quality scores are stored within the FASTQ files as ASCII
characters

```{r, eval=TRUE,echo=FALSE, , fig.cap='Phred quality scores as ASCII characters', out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/ascii.png")
```

This is all stored together as four simple lines of repeating text so
that a FASTQ file containing a single sequence might look like this:

```         
@SEQ_ID
GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT
+
!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65
```

A single FASTQ file may contain millions of sequencing reads. Let's look
at the first 40 lines of *one* of our FASTQ files. And check it looks
like a standard format.

```{bash}
head -40 B1_sub_F1.fq
```

<details>

<summary>**Q. By eye, can you tell whether these few lines look to be of
good quality? - Click here for Answer**</summary>

*The ASCII characters are repeated every fourth line, most of these
reads appear to be G letters or close to this - indicating greater than
99.9% accuracy - pretty good. Very observant students might have noticed
that the end of the reads appear to be of lower quality. More on this
later*

</details>

## The Pipeline

This is a very simple overview of the pipeline we will run, some of
these steps (especially early ones - are applicable to lots of NGS
applications), later on they become more specific to *our* data.

-   Import the FASTQ files and demultiplex (this step was done for us)

-   Remove adapters and primers (these may be included with our reads,
    but they are not part of the *natural* DNA sequence)

-   Check FASTQ data quality and trim/filter reads accordingly

-   Dereplicate (collapse identical sequences and choose a
    representative)

-   Assign ASVs - decide if (non-identical) sequences are similar enough
    to be considered as from the same species

-   Join forward and reverse reads together

-   Assign ASVs to Taxonomies

-   Count the abundance of different ASVs

-   Export taxonomy file, ASV fasta sequence file and count file to R
    for analysis

## Removing Primers

To start, we need to remove the primers from all of these (the primers
used for this run are in the "primers.fa" file in our working
directory), and here we're going to use `cutadapt` to do that at the
command line ("Terminal" tab).

First we need to install `cutadapt`

```{bash}
python3 -m pip install --user --upgrade cutadapt
```

> **Note**
>
> You will probably get a warning message about PATH. You can ignore
> this, what it means is that CUTADAPT has been installed in your home
> directory, in order to use it we need to specify the absolute path TO
> cutadapt when we call it. This is done for you in the next code box
> below.

Cutadapt operates on one sample at at time, so we're going to use a
wonderful little bash *loop* to run it on all of our samples.

### Loops

Loops are extremely powerful way of controlling iteration. We can
specify that a line of code is repeated across multiple objects. In this
example we use the `samples` file we made earlier as the list of files
across which we want this function of removing primers to loop. These
same lines will then repeat until all the specified iterations are
complete.

We won't break down exactly how this loop works - but they are used
across all programming languages (including R) and you can check out the
R4DS book for an introduction to building your own loops (and custom
functions!) [here](https://r4ds.had.co.nz/iteration.html).

For now just copy and paste this code exactly into the Terminal.

```{bash}
    for sample in $(cat samples)
    do

        echo "On sample: $sample"
        
        ~/.local/bin/cutadapt -a ^GTGCCAGCMGCCGCGGTAA...ATTAGAWACCCBDGTAGTCC \
        -A ^GGACTACHVGGGTWTCTAAT...TTACCGCGGCKGCTGGCAC \
        -m 215 -M 285 --discard-untrimmed \
        -o ${sample}_sub_R1_trimmed.fq.gz -p ${sample}_sub_R2_trimmed.fq.gz \
        ${sample}_sub_R1.fq ${sample}_sub_R2.fq \
        >> cutadapt_primer_trimming_stats.txt 2>&1

    done
```

Here's a before-and-after of one of our files - if you look at the
sequences supplied: GTGCCAGCMGCCGCGGTAA...ATTAGAWACCCBDGTAGTCC these
indicate the forward primer and the reverse primer (our amplicon is
everything inbetween). If you look at our before and after you should
see these were at the start and end of the sequence but have now been
trimmed off.

```         
### R1 BEFORE TRIMMING PRIMERS
head -n 2 B1_sub_R1.fq
# @M02542:42:000000000-ABVHU:1:1101:8823:2303 1:N:0:3
# GTGCCAGCAGCCGCGGTAATACGTAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTCTTGT
# AAGACAGAGGTGAAATCCCTGGGCTCAACCTAGGAATGGCCTTTGTGACTGCAAGGCTGGAGTGCGGCAGAGGGGGATGG
# AATTCCGCGTGTAGCAGTGAAATGCGTAGATATGCGGAGGAACACCGATGGCGAAGGCAGTCCCCTGGGCCTGCACTGAC
# GCTCATGCACGAAAGCGTGGGGAGCAAACAGGATTAGATACCCGGGTAGTCC

### R1 AFTER TRIMMING PRIMERS
head -n 2 B1_sub_R1_trimmed.fq
# @M02542:42:000000000-ABVHU:1:1101:8823:2303 1:N:0:3
# TACGTAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTCTTGTAAGACAGAGGTGAAATCCC
# TGGGCTCAACCTAGGAATGGCCTTTGTGACTGCAAGGCTGGAGTGCGGCAGAGGGGGATGGAATTCCGCGTGTAGCAGTG
# AAATGCGTAGATATGCGGAGGAACACCGATGGCGAAGGCAGTCCCCTGGGCCTGCACTGACGCTCATGCACGAAAGCGTG
# GGGAGCAAACAGG
```

You can look through the output of the cutadapt stats file we made
("cutadapt_primer_trimming_stats.txt") to get an idea of how things
went.

Here's a little one-liner to look at what fraction of reads were
retained in each sample (column 2) and what fraction of bps were
retained in each sample (column 3):

```{bash}
    paste samples <(grep "passing" cutadapt_primer_trimming_stats.txt | cut -f3 -d "(" | tr -d ")") <(grep "filtered" cutadapt_primer_trimming_stats.txt | cut -f3 -d "(" | tr -d ")")
```

```         
# B1    96.5%   83.0%
# B2    96.6%   83.3%
# B3    95.4%   82.4%
# B4    96.8%   83.4%
# BW1   96.4%   83.0%
# BW2   94.6%   81.6%
# R10   92.4%   79.8%
# R11BF 90.6%   78.2%
# R11   93.3%   80.6%
# R12   94.3%   81.4%
# R1A   93.3%   80.5%
# R1B   94.0%   81.1%
# R2    94.0%   81.2%
# R3    93.8%   81.0%
# R4    95.5%   82.4%
# R5    93.7%   80.9%
# R6    92.7%   80.1%
# R7    94.4%   81.5%
# R8    93.2%   80.4%
# R9    92.4%   79.7%
```

This looks like it worked pretty well! Some reads were discarded
entirely `-m 215 -M 285 --discard-untrimmed \` anything \<215bp or
\>285bp was discarded. Looks like c. 6-8% We lost a greater proportion
of bp overall, but this is the program working as it should, making most
of our reads a little shorter as it cuts off the primers. Overall it
looks like we lost c.20% of bp.

Importantly all of our files have behaved *roughly* the same.

With primers removed, we're now ready to switch R and start using DADA2!

## DADA2

```{block, type="rmdwarning"}

Switch from the Terminal to Console now.
We are working in R for the rest of this workflow

```

```{r}
    library(dada2)


    setwd("dada2_amplicon_ex_workflow")

    list.files() # make sure what we think is here is actually here

    ## first we're setting a few R objects we're going to use ##
      # one with all sample names, by scanning our "samples" file we made earlier
  samples <- scan("samples", what="character")

      # one holding the file names of all the forward reads
  forward_reads <- paste0(samples, "_sub_R1_trimmed.fq.gz")

      # and one with the reverse
  reverse_reads <- paste0(samples, "_sub_R2_trimmed.fq.gz")

      # and variables holding file names for the forward and reverse
      # filtered reads we're going to generate below
  filtered_forward_reads <- paste0(samples, "_sub_R1_filtered.fq.gz")
  filtered_reverse_reads <- paste0(samples, "_sub_R2_filtered.fq.gz")
  
```

## Quality trimming/filtering

We did a filtering step above with cutadapt (where we eliminated reads
that had imperfect or missing primers and those that were shorter than
215 bps or longer than 285), but in DADA2 we'll implement a trimming
step as well (where we trim reads down based on some quality threshold
rather than throwing the read away).

Since we're potentially shortening reads further, we're again going to
include another minimum-length filtering component. We can also take
advantage of a handy quality plotting function that DADA2 provides to
visualize how you're reads are doing, plotQualityProfile().

By running that on our variables that hold all of our forward and
reverse read filenames, we can easily generate plots for all samples or
for a subset of them. So let's take a peak at that to help decide our
trimming lengths: It's good to try to keep a bird's-eye view of what's
going on. So here is an overview of the main processing steps we'll be
performing with cutadapt and DADA2. Don't worry if anything seems
unclear right now, we will discuss each at each step.

```{r}
    plotQualityProfile(forward_reads[17:20])

    plotQualityProfile(reverse_reads[17:20])
```

All forwards look pretty similar to each other, and all reverses look
pretty similar to each other, but worse than the forwards, which is
common -- the Illumina sequencer reads all of the molecules in the
forward orientation *first*, then the clusters are flipped and read in
reverse. But this means a lot of the chemical reagents start to get used
up or degraded, so it is usual to get lower quality reverse reads.

On the plots produced

-   the x axis is the nucleotide bases starting from the beginning of
    the read moving to the end

-   the y axis is the average quality score for the base in that
    position

-   the green line is the median quality score of the base at that
    position

-   the orange lines are quartiles

These quality profiles are based entirely on taking the *average* PHRED
scores for sequences at that position in the sample

Here, I'm going to cut the forward reads at 250 and the reverse reads at
200 -- roughly where both sets maintain a median quality of 30 or above
-- and then see how things look. But we also want to set a minimum
length to filter out truncated sequences, so we will set a minimum
acceptable read length of 175bp (any reads shorter than this will be
discarded).

In DADA2, this quality-filtering step is done with the `filterAndTrim()`
function:

```{r}

    filtered_out <- filterAndTrim(forward_reads, filtered_forward_reads,
                    reverse_reads, filtered_reverse_reads, maxEE=c(2,2),
                    rm.phix=TRUE, minLen=175, truncLen=c(250,200))
```

This function made a bunch of output files "filtered_forward_reads" and
"filtered_reverse_reads" we can see these in our project pane. Or if we
were working on a server without a GUI we could use `list.files()` in R
or `ls` in our Terminal.

We also generated an R file called filtered_out. This is a simple matrix
holding how many reads went *in* for each file and how many came *out*.

Check it in R.

```{r}
    filtered_out
```

We can take a look at the filtered reads visually - we expect to have
trimmed off that section where quality drops

```{r}
    plotQualityProfile(filtered_reverse_reads[17:20])
```

Looking Good!

## Dereplication

Dereplication is a common step in many amplicon processing workflows.
Instead of keeping 100 identical sequences and doing all downstream
processing to all 100 -costing computer processing power and time, you
can keep/process just one of them, and just attach the number x100 to
it. Now this acts as a representative for 100 identical sequences.

```{r}
derep_forward <- derepFastq(filtered_forward_reads, verbose=TRUE)
names(derep_forward) <- samples # the sample names in these objects are initially the file names of the samples, this sets them to the sample names for the rest of the workflow
derep_reverse <- derepFastq(filtered_reverse_reads, verbose=TRUE)
names(derep_reverse) <- samples
```

## ASV's

This is where we start to take our raw sequence data and infer *true*
biological sequences. It uses an algorithm to look at the consensus
quality score and abundance for each *unique* sequence. It then
determines whether this sequence is more likely to be of biological
origin or a spurious sequencing error.

> **Note**
>
> This step may take a few minutes to run, so be patient!

```{r}
load("amplicon_dada2_ex.RData")

dada_forward <- dada(derep_forward, err=err_forward_reads, pool="pseudo")

dada_reverse <- dada(derep_reverse, err=err_reverse_reads, pool="pseudo")


```

## Merging reads

Now DADA2 merges the forward and reverse ASVs to reconstruct our full
target amplicon requiring the overlapping region to be identical between
the two. By default it requires that at least 12 bps overlap, but in our
case the overlap should be much greater. If you remember above we
trimmed the forward reads to 250 and the reverse to 200, and our primers
were 515f--806r. After cutting off the primers we're expecting a typical
amplicon size of around 260 bases, so our typical overlap should be up
around 190. That's estimated based on E. coli 16S rRNA gene positions
and very back-of-the-envelope-esque of course, so to allow for true
biological variation and such I'm going ot set the minimum overlap for
this dataset for 170. I'm also setting the trimOverhang option to TRUE
in case any of our reads go passed their opposite primers (which I
wouldn't expect based on our trimming, but is possible due to the region
and sequencing method).

```{r}
merged_amplicons <- mergePairs(dada_forward, derep_forward, dada_reverse,
                    derep_reverse, trimOverhang=TRUE, minOverlap=170)

  # this object holds a lot of information that may be the first place you'd want to look if you want to start poking under the hood
class(merged_amplicons) # list
length(merged_amplicons) # 20 elements in this list, one for each of our samples
names(merged_amplicons) # the names() function gives us the name of each element of the list 

class(merged_amplicons$B1) # each element of the list is a dataframe that can be accessed and manipulated like any ordinary dataframe

names(merged_amplicons$B1) # the names() function on a dataframe gives you the column names
# "sequence"  "abundance" "forward"   "reverse"   "nmatch"    "nmismatch" "nindel"    "prefer"    "accept"
```

## Count table

Now we can generate a count table with the makeSequenceTable() function.
This is one of the main outputs from processing an amplicon dataset. You
may have also heard this referred to as a biome table, or an OTU matrix.

```{r}
seqtab <- makeSequenceTable(merged_amplicons)
class(seqtab) # matrix
dim(seqtab) # 20 2521

```

## Overview

The developers' DADA2 tutorial provides an example of a nice, quick way
to pull out how many reads were dropped at various points of the
pipeline. This can serve as a jumping off point if you're left with too
few sequences at the end to help point you towards where you should
start digging into where they are being dropped. Here's a slightly
modified version:

```{r}
  # set a little function
getN <- function(x) sum(getUniques(x))

  # making a little table
summary_tab <- data.frame(row.names=samples, dada2_input=filtered_out[,1],
               filtered=filtered_out[,2], dada_f=sapply(dada_forward, getN),
               dada_r=sapply(dada_reverse, getN), merged=sapply(merged_amplicons, getN),
               nonchim=rowSums(seqtab.nochim),
               final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 1))

summary_tab

```

And it might be useful to write this table out of R, saving it as a
regular file

```{r}
write.table(summary_tab, "read-count-tracking.tsv", quote=FALSE, sep="\t", col.names=NA)
```

## Assign taxonomy

```{block, type="rmdwarning"}

Running the Taxonomy assignment step below can take anywhere from 30 minutes to a few hours depending on how much RAM we provide.
So for this example run - we will skip this step and load an R.data file which has this information in it already

```

```{r}
load("amplicon_dada2_ex.RData")
```

<details>

<summary>**Example code for running taxonomy assignment - Click
here**</summary>

*So we won't run this code in this example, but here it is for
reference.*

```         
## downloading DECIPHER-formatted SILVA v138 reference
download.file(url="http://www2.decipher.codes/Classification/TrainingSets/SILVA_SSU_r138_2019.RData", destfile="SILVA_SSU_r138_2019.RData")

## loading reference taxonomy object
load("SILVA_SSU_r138_2019.RData")

## loading DECIPHER
library(DECIPHER)

## creating DNAStringSet object of our ASVs
dna <- DNAStringSet(getSequences(seqtab.nochim))

## and classifying
tax_info <- IdTaxa(test=dna, trainingSet=trainingSet, strand="both", processors=NULL)
```

</details>

## Standard goods

The typical standard outputs from amplicon processing are

-   a fasta file: each ASV represented by a sequence
    `asv_fasta_no_contam`

-   a count table: how many sequences of each ASV in each sample
    `asv_tab_no_contam`

-   a taxonomy file: the closest biological species to the fasta
    sequence `asv_tax_no_contam`

These objects from DADA2 can then be analysed to start to understand the
different bacterial communities from our deep-sea study:

> **Note**
>
> -   These three files are relatively small simply lists now, you can
>     type them into the R console and inspect these outputs if you
>     wish. Do they make sense to you?

```{r, include=FALSE}
  # giving our seq headers more manageable names (ASV_1, ASV_2...)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

  # making and writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs.fa")

  # count table:
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

  # tax table:
  # creating table of taxonomy and setting any that are unclassified as "NA"
ranks <- c("domain", "phylum", "class", "order", "family", "genus", "species")
asv_tax <- t(sapply(tax_info, function(x) {
  m <- match(ranks, x$rank)
  taxa <- x$taxon[m]
  taxa[startsWith(taxa, "unclassified_")] <- NA
  taxa
}))
colnames(asv_tax) <- ranks
rownames(asv_tax) <- gsub(pattern=">", replacement="", x=asv_headers)

write.table(asv_tax, "ASVs_taxonomy.tsv", sep = "\t", quote=F, col.names=NA)

```

## Functions list

```{r, echo=FALSE, eval=TRUE}
terms <- read_csv("data/dada.csv", skip_empty_rows = TRUE) 

knitr::kable(terms, booktabs = TRUE,
             col.names = c("Command", "What it is"),
             longtable=TRUE) %>%
  kableExtra::row_spec(0, bold = TRUE) %>%
  kableExtra::kable_styling()
```

## Summary

We have imported FASTQ data from an Illumina sequencing run, processed
the files to remove poor quality reads and trim primers. We have then
put this through a microbiome specific bioinformatics pipeline to assign
millions of individual reads to more manageable representative
sequences. We have assigned taxonomies to these sequences and tallied
them, so that **next time** we can actually inspect our data and start
to make visuals that describe our microbial communities.
